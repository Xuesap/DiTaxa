{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "from utility.file_utility import FileUtility\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utility.math_utility import normalize_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxonomy_gg_all=FileUtility.load_list('../../16S_general_data/GG/ids_min20.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n",
      "class\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n",
      "order\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n",
      "phylum\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n",
      "genus\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n",
      "species\n",
      "start corpus making\n",
      "end corpus making\n",
      "normalization\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "levels={'phylum':2,'class':3,'order':4,'family':5,'genus':6,'species':7}\n",
    "for level in levels:\n",
    "    print (level)\n",
    "    \n",
    "    ## making a dic for this level going from tax-id to list or rows in the segmented file\n",
    "    taxidx_numidx=[(''.join(line.split()[1:(levels[level]+1)]),idx) for idx, line in enumerate(taxonomy_gg_all)]\n",
    "    taxonomy_ids=dict()\n",
    "    for tax_id, num_idx in taxidx_numidx:\n",
    "        if tax_id not in taxonomy_ids:\n",
    "            taxonomy_ids[tax_id]=list()\n",
    "        taxonomy_ids[tax_id].append(num_idx)\n",
    "    \n",
    "    ## loading the segmented file\n",
    "    sequences = FileUtility.load_list('../../16S_general_data/GG/segmented_corpus_min20.txt')\n",
    "\n",
    "    ## make a corpus (each line is a distinct taxonomy) and all merged sequences for that\n",
    "    print('start corpus making')\n",
    "    texts=[]\n",
    "    taxonomy_list=[]\n",
    "    for taxa, list_id in list(taxonomy_ids.items()):\n",
    "        seqs=[sequences[idx] for idx in list_id]\n",
    "        texts.append(' '.join(seqs))\n",
    "        taxonomy_list.append(taxa)\n",
    "    print('end corpus making')\n",
    "    \n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, analyzer='word',\n",
    "                                            norm=None, stop_words=[], lowercase=True, binary=False, smooth_idf=False, tokenizer=str.split)\n",
    "    tf_vec = tf_vectorizer.fit_transform(texts)\n",
    "    feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    print ('normalization')\n",
    "    normalize_matrix=normalize_mat(tf_vec,axis=0)\n",
    "    normalize_matrix=normalize_matrix.toarray()\n",
    "    prob=[' '.join(['###'.join([taxonomy_list[x],str(np.round(normalize_matrix[x,i],5))]) for x in np.nonzero(normalize_matrix[:,i])[0].tolist() if np.round(normalize_matrix[x,i],5) > 0.1]) for i in range(normalize_matrix.shape[1])]\n",
    "    print ('saving')\n",
    "    FileUtility.save_sparse_csr('../../16S_general_data/GG//intermediate/gg_tf_'+level+'_mat', tf_vec)\n",
    "    FileUtility.save_list('../../16S_general_data/GG/intermediate/gg_'+level,taxonomy_list)\n",
    "    FileUtility.save_list('../../16S_general_data/GG/intermediate/gg_cpe_'+level+'_prob',prob)\n",
    "    FileUtility.save_list('../../16S_general_data/GG/intermediate/gg_cpe_'+level+'_features',feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2taxa=dict()\n",
    "for level in levels:\n",
    "    prob=FileUtility.load_list('../../16S_general_data/GG/intermediate/gg_cpe_'+level+'_prob')\n",
    "    features=FileUtility.load_list('../../16S_general_data/GG/intermediate/gg_cpe_'+level+'_features')\n",
    "    for f_idx,feature in enumerate(features):\n",
    "        if feature not in seq2taxa:\n",
    "            seq2taxa[feature]=dict()\n",
    "        if level not in seq2taxa[feature]:\n",
    "            seq2taxa[feature][level]=[]\n",
    "        seq2taxa[feature][level]+=[(pairs.split('###')[0],float(pairs.split('###')[1]))for pairs in prob[f_idx].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FileUtility.save_obj('../../16Seq2Seg/data_config/seq2tax.pickle', seq2taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': [('k__Archaea;p__Euryarchaeota;c__Halobacteria;', 0.96667)],\n",
       " 'family': [('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;f__Halobacteriaceae;',\n",
       "   0.96667)],\n",
       " 'genus': [('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;f__Halobacteriaceae;g__Halobacterium;',\n",
       "   0.38333),\n",
       "  ('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;f__Halobacteriaceae;g__Halorhabdus;',\n",
       "   0.5)],\n",
       " 'order': [('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;',\n",
       "   0.96667)],\n",
       " 'phylum': [('k__Archaea;p__Euryarchaeota;', 0.96667)],\n",
       " 'species': [('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;f__Halobacteriaceae;g__Halorhabdus;s__',\n",
       "   0.5),\n",
       "  ('k__Archaea;p__Euryarchaeota;c__Halobacteria;o__Halobacteriales;f__Halobacteriaceae;g__Halobacterium;s__',\n",
       "   0.38333)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2taxa['aagtccgttgg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
